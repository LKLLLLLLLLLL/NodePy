import os
from typing import AsyncIterator, Iterator

from sqlalchemy import (
    JSON,
    BigInteger,
    Boolean,
    Column,
    DateTime,
    Enum,
    ForeignKey,
    Integer,
    LargeBinary,
    String,
    UniqueConstraint,
    create_engine,
    text,
)
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.orm.session import Session
from sqlalchemy.sql import func

"""
Session management.
"""

# sync engine and session
DATABASE_URL = os.getenv("DATABASE_URL", "")
engine = create_engine(DATABASE_URL, pool_size=20, max_overflow=40, pool_timeout=30)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
def get_session() -> Iterator[Session]:
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# async engine and session
ASYNC_DATABASE_URL = DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")
async_engine = create_async_engine(ASYNC_DATABASE_URL, echo=False, pool_pre_ping=True)
AsyncSessionLocal = async_sessionmaker(
    bind=async_engine, 
    class_=AsyncSession, 
    expire_on_commit=False
)
async def get_async_session() -> AsyncIterator[AsyncSession]:
    async with AsyncSessionLocal() as session:
        yield session 

"""
The models used in database.
"""
Base = declarative_base()
class UserRecord(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True, autoincrement=True)
    username = Column(String, unique=True, index=True, nullable=False)
    email = Column(String, unique=True, index=True, nullable=False)
    # Three types of authentication: password, google oauth, github oauth
    hashed_password = Column(String, nullable=True)
    google_id = Column(String, unique=True, index=True, nullable=True)  # OAuth IDs
    github_id = Column(String, unique=True, index=True, nullable=True)

    file_total_space = Column(BigInteger, default=5 * 1024 * 1024 * 1024, nullable=False)  # 5 GB default
    created_at = Column(DateTime(timezone=True), nullable=False, server_default=func.now())

class ProjectRecord(Base):
    __tablename__ = "projects"

    id = Column(Integer, primary_key=True, index=True, autoincrement=True)
    name = Column(String, unique=False, index=True, nullable=False)
    owner_id = Column(Integer, ForeignKey("users.id", ondelete="CASCADE"), nullable=False)
    workflow = Column(JSON, nullable=False)  # Serialized workflow structure
    ui_state = Column(JSON, nullable=False)  # Serialized UI state
    thumb = Column(LargeBinary, nullable=True)
    created_at = Column(DateTime(timezone=True), nullable=False, server_default=func.now())
    updated_at = Column(DateTime(timezone=True), nullable=False, server_default=func.now(), onupdate=func.now())
    show_in_explore = Column(Boolean, default=False, nullable=False)

    __table_args__ = (
        UniqueConstraint('owner_id', 'name', name='_owner_name_uc'),
    )

class NodeOutputRecord(Base):
    """
    The output data generated by nodes.
    """
    __tablename__ = "data"
    
    id = Column(Integer, primary_key=True, index=True, autoincrement=True)
    project_id = Column(
        Integer, ForeignKey("projects.id", ondelete="CASCADE"), nullable=False
    )
    node_id = Column(String, nullable=False, index=True) # reference the node id in project.graph.nodes[i].id
    port = Column(String, nullable=False)  # output port name
    data = Column(JSON, nullable=False)  # Arbitrary JSON data

class FileRecord(Base):
    __tablename__ = "files"

    id = Column(Integer, primary_key=True, index=True, autoincrement=True)
    filename = Column(String, nullable=False)  # Original file name
    file_key = Column(String, nullable=False) # MinIO object key
    format = Column(Enum("jpg", "png", "csv", "pdf", name="file_format"), nullable=False)
    user_id = Column(Integer, ForeignKey("users.id", ondelete="CASCADE"), nullable=False)
    project_id = Column(Integer, ForeignKey("projects.id", ondelete="CASCADE"), nullable=False)
    node_id = Column(String, nullable=False, index=True) # ID of the node that generated the file
    file_size = Column(BigInteger, nullable=False) # Byte
    last_modify_time = Column(DateTime(timezone=True), nullable=False, server_default=func.now(), onupdate=func.now())
    
    __table_args__ = (
        UniqueConstraint('project_id', 'node_id', name='_project_node_uc'),
    )

# trigers
def file_size_trigger(conn) -> None:
    conn.execute(
        text("""
        CREATE OR REPLACE FUNCTION check_user_storage_limit() RETURNS trigger AS $$
        DECLARE
            total_occupy BIGINT;
            user_limit BIGINT;
        BEGIN
            SELECT COALESCE(SUM(file_size), 0) INTO total_occupy
            FROM files
            WHERE user_id = NEW.user_id;
            SELECT file_total_space INTO user_limit
            FROM users
            WHERE id = NEW.user_id;
            IF total_occupy > user_limit THEN
                RAISE EXCEPTION 'User storage limit exceeded';
            END IF;
            RETURN NEW;
        END;
        $$ LANGUAGE plpgsql;
    """)
    )
    conn.execute(
        text("""
        DROP TRIGGER IF EXISTS files_check_storage_limit_before ON files;
        CREATE TRIGGER files_check_storage_limit_before
        BEFORE INSERT OR UPDATE ON files
        FOR EACH ROW EXECUTE FUNCTION check_user_storage_limit();
    """)
    )
    conn.commit()

def data_cleanup_trigger(conn) -> None:
    conn.execute(
        text("""
        CREATE OR REPLACE FUNCTION cleanup_data_on_project_change() RETURNS trigger AS $$
        DECLARE
            node_ids TEXT[];
        BEGIN
            -- extract all node ids from workflow
            SELECT ARRAY_AGG(node->>'id') INTO node_ids
            FROM JSON_ARRAY_ELEMENTS(NEW.workflow->'nodes') AS node;

            -- delete data that belongs to the project and whose node_id is not in the node_ids list
            DELETE FROM data
            WHERE project_id = NEW.id
              AND node_id IS NOT NULL
              AND node_id <> ALL(node_ids);

            RETURN NEW;
        END;
        $$ LANGUAGE plpgsql;
        """)
    )
    conn.execute(
        text("""
        DROP TRIGGER IF EXISTS cleanup_data_after_project_update ON projects;
        CREATE TRIGGER cleanup_data_after_project_update
        AFTER UPDATE OF workflow ON projects
        FOR EACH ROW
        EXECUTE FUNCTION cleanup_data_on_project_change();
        """)
    )
    conn.commit()

def init_database() -> None:
    """ Initialize the database: create tables and triggers """
    # Create database tables
    Base.metadata.create_all(bind=engine)
    # Create triggers
    with engine.connect() as conn:
        file_size_trigger(conn)
        data_cleanup_trigger(conn)

class DatabaseTransaction:
    """
    A context manager for a database transaction.
    Usage:
        with DatabaseTransaction() as db:
            ...
    """
    def __init__(self):
        self.db: Session | None = None

    def __enter__(self) -> Session:
        self.db = SessionLocal()
        return self.db

    def __exit__(self, exc_type, exc_value, traceback) -> None:
        if self.db:
            if exc_type is not None:
                self.db.rollback()
            self.db.close()